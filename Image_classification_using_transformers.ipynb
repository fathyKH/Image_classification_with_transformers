{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STA6-YmP6K0u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as n\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "0pI453A_W-bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Image to Patches**"
      ],
      "metadata": {
        "id": "amPyczdxe-Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class patch_embedding():\n",
        "  def __init__(self, patch_size):\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "  def __call__(self, img):\n",
        "    patches = img.unfold(1,self.patch_size,self.patch_size)\n",
        "    patches = img.unfold(2,self.patch_size,self.patch_size)\n",
        "    patches = patches.reshape(-1,self.patch_size*self.patch_size)\n",
        "    return patches"
      ],
      "metadata": {
        "id": "rK9oBXRYIkEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "XrkNR4PLf30Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patch_size = 4\n",
        "transform = transforms.Compose(\n",
        "    [transforms.RandomHorizontalFlip(),\n",
        "     transforms.RandomResizedCrop((28,28)),\n",
        "     transforms.RandomRotation(90),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,), (0.5,)),\n",
        "     patch_embedding(patch_size)])"
      ],
      "metadata": {
        "id": "1CT5HV5AgTQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                               train=True,\n",
        "                               transform=transform,\n",
        "                               download=True)\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                              train=False,\n",
        "                              transform=transform)"
      ],
      "metadata": {
        "id": "GR2HRcL5-h_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "j6kk7aaPu5lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.Linear(16,512)(train_dataset[0][0]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZuqRiLZcks7",
        "outputId": "64bec774-1920-4f0f-868f-cb78ee2917aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([49, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Patch Embedding**"
      ],
      "metadata": {
        "id": "gTIcQWZGdDYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEmbeddings(n.Module):\n",
        "  def __init__(self, size,patch_size,hidden_size,num_patches):\n",
        "    super().__init__()\n",
        "    self.projection = n.Linear(size, hidden_size)\n",
        "    self.class_token = n.Parameter(torch.rand(1,hidden_size))\n",
        "    self.position_embedding = n.Parameter(torch.rand(1,num_patches+1,hidden_size))\n",
        "\n",
        "    self.dropout = n.Dropout(0.5)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.projection(x)\n",
        "    class_token = self.class_token.expand(x.shape[0],-1,-1)\n",
        "    x = torch.cat((class_token,x),dim=1)\n",
        "\n",
        "    position_embedding = self.position_embedding.expand(x.shape[0],-1,-1)\n",
        "\n",
        "    x = x + position_embedding\n",
        "    x = self.dropout(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "oHqE6rCLdxvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation of Attention**"
      ],
      "metadata": {
        "id": "JACrPbehde9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HeadAttention(n.Module):\n",
        "  def __init__(self, hidden_size):\n",
        "    super().__init__()\n",
        "    self.query = n.Linear(hidden_size, hidden_size)\n",
        "    self.key = n.Linear(hidden_size, hidden_size)\n",
        "    self.value = n.Linear(hidden_size, hidden_size)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    query = self.query(x)\n",
        "    key = self.key(x)\n",
        "    value = self.value(x)\n",
        "    scale = query.shape[-1] ** -0.5\n",
        "\n",
        "    attention= torch.bmm(query,key.transpose(-1,-2))/scale\n",
        "    attention = torch.softmax(attention,dim=-1)\n",
        "    attention = torch.bmm(attention,value)\n",
        "\n",
        "    return attention"
      ],
      "metadata": {
        "id": "PACbbYBsJAzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(n.Module):\n",
        "  def __init__(self, hidden_size, num_heads):\n",
        "    super().__init__()\n",
        "    self.heads = n.ModuleList([HeadAttention(hidden_size) for _ in range(num_heads)])\n",
        "    self.linear = n.Linear(hidden_size*num_heads, hidden_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = [head(x) for head in self.heads]\n",
        "    x = torch.cat(x,dim=-1)\n",
        "    x = self.linear(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "oyDc8xBCdlSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Encoder Block**"
      ],
      "metadata": {
        "id": "DTkJV0-Mdul-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(n.Module):\n",
        "  def __init__(self, hidden_size, num_heads):\n",
        "    super().__init__()\n",
        "    self.attention = MultiHeadAttention(hidden_size, num_heads)\n",
        "    self.normAttention = n.LayerNorm(hidden_size)\n",
        "    self.normMLP = n.LayerNorm(hidden_size)\n",
        "    self.mlp = n.Sequential(\n",
        "        n.Linear(hidden_size, 4*hidden_size),\n",
        "        n.Dropout(0.5),\n",
        "        n.LeakyReLU(),\n",
        "        n.Linear(4*hidden_size, hidden_size),\n",
        "        n.Dropout(0.5)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attention(self.normAttention(x))\n",
        "    x = x + self.mlp(self.normMLP(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "g5H4boWAi1Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the Transformer Model**"
      ],
      "metadata": {
        "id": "lIwWRnZEd1c9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vit(n.Module):\n",
        "  def __init__(self, hidden_size, patch_size, num_patches, num_heads,num_classes):\n",
        "    super().__init__()\n",
        "    self.image_embedding = ImageEmbeddings(size=patch_size,patch_size=patch_size,hidden_size=hidden_size,num_patches=num_patches)\n",
        "    self.encoders = n.Sequential(*[EncoderBlock(hidden_size, num_heads) for _ in range(1)])\n",
        "    self.mlp = n.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.image_embedding(x)\n",
        "    x = self.encoders(x)\n",
        "    #only output the class_token layer\n",
        "    x = self.mlp(x[:,0,:])\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Y3DxWtq_ezds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_heads = 6\n",
        "hidden_size = 128\n",
        "num_patches = 49\n",
        "num_classes = 10\n",
        "patch_size = 16\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "RtW-HNZqcXoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the model**"
      ],
      "metadata": {
        "id": "b7IMDevSeJFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = n.CrossEntropyLoss()\n",
        "model = Vit(hidden_size, patch_size, num_patches, num_heads,num_classes)\n",
        "model.to('cuda')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-2)\n",
        "torch.set_float32_matmul_precision('medium')\n",
        "def train(epochs):\n",
        "  model.train()\n",
        "  for _ in range(epochs):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      data, target = data.to('cuda'), target.to('cuda')\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      print(loss)\n",
        "    print('epoch {} finished:'.format(_))\n"
      ],
      "metadata": {
        "id": "Lj6w24O-pe0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(num_epochs)"
      ],
      "metadata": {
        "id": "QpDzGy0KvnNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the model**"
      ],
      "metadata": {
        "id": "m4lD_0Z4eUYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to('cuda'), target.to('cuda')\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "accuracy = correct / len(test_loader.dataset)\n",
        "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    correct, len(test_loader.dataset),\n",
        "    100. * accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJCrXAiF2E9j",
        "outputId": "818b9c00-ec34-48e7-880a-204544d61b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Accuracy: 1770/10000 (18%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}